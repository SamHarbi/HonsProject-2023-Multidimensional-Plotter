\section{Evaluation}
An important driving factor for this project and the application developed was a system of Evaluation at all levels of development and planning. To that end, the student employed a
number of techniques and phases to convert feedback into actionable input that would then influence how the project progressed. Each of those techniques is covered in detail in the following subsections.

\subsection{Development Testing Process}
Under PXP, the refactor branch was adapted to also serve as a final testing stop before the student pushed code to production (See specification for more detail on PXP). Automated testing was considered as mentioned in the Design section but was found to not be suitable for this project due to the heavy focus on rendering development.
A technology review of some options was considered in the Design section but a suitable option was not found.

Instead, refactoring was done manually on the refactoring branch based on visual inspection for rendering artifacts. To help with this process, a python command line tool was written to generate a wide range of datasets for edge case testing. The datasets are all attached in appendix X but some notable ones are as follows:

\begin{itemize}
    \item DataCube - This is a data set that creates a 10x10x10 Cube of data points. If the application renders correctly, this dataset would fill every visible space at the initial zoom and slice position.
    \item Terrain Data - This is a Stress test dataset consisting of perlin noise generated terrain coordinates in a 25x25x12 sized chunk. It is a test for not only performance but also ability to handle data points with decimal values.
\end{itemize}

For UI testing, It was considered by the student that there was minimal value based on how little UI the project had for the majority of development, In only the last 2 sprints did the UI develop to a point where automated testing could be applied beyond it's simplest state. See the design section for further analysis on this during the start of the project.

These datasets also served to provide a wide range of example data for user testers to use.

\subsection{User Testing} \label{usertest1}
User testing was an important step in gathering requirements and paving the way for further development beyond the most basic, fundamental requirements. In total, 2 User testing phases were ran. The first one after the completion of the MVP feature set post Sprint 5 and the second one after the completion of the further additions feature set and user testing 1 feature set post sprint 8.

The process for these two phases revolved around an anonymous questionnaire with data collected as per the prepared ethics documentation for honours students.
To support the testing process, the student created a simple static webpage to act as a central source for all links to the application, the questionnaire, testing data and ethics documentation that guided a tester through the entire process. Having this page really simplified testing as the student would only need to share a single link which self-contained everything needed. The page itself and HTML source is available to view in the appendix X.

The questionnaire itself (See appendix X) asked a tester to download a specific dataset and use the application to identify some factor or knowledge placed in the data by the student. The datasets were also designed in such a way that the questions could only be solved if the tester used some specific controls, or understood some aspect of the graph. This was also followed by the testers personal thoughts on a variety of questions concerning usability and how they went about solving the problems set out.

This allowed the student to construct a critical understanding of how real users used the application to solve typical problems the application was designed in mind with. Were users able to get the correct answer? If not, did they think they got a correct answer? What controls did they use? Could those Controls have misinformed them or gave them a false understanding of the data?

Though it should be mentioned that this analysis likely is not fully representative of the demographic tested due to the limited number of testers. Of which, a majority were computing students. This likely shifted results for usability specifically.

The specifics of responses and analysis carried out leading to the creation of Feature sets are covered in more detail below for each of the testing phases undertaken.


\subsubsection{User Testing 1 - Starting February 17th}
This was setup and ran right after sprint 5 was complete. The focus for this testing phase was to identify what non-obvious controls are still missing to help users do real work using the application. A questionnaire was created to that end and user testing began.
The user testing was then concluded a week later with 5 total responses.

\subsubsection{User Testing 2 - Starting March 30th}
TODO WIP

